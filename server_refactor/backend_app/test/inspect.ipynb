{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def impute(command):\n",
    "    response = {'text': \"\"}\n",
    "    command_parts = [part for part in command.split(\" \") if part.strip()]\n",
    "    table_name = command_parts[command_parts.index(\"FROM\") + 1].split(';')[0]\n",
    "    features = command_parts[command_parts.index(\"IMPUTE\") + 1]\n",
    "    strat = command_parts[command_parts.index(\"STRATEGY\") + 1] if \"STRATEGY\" in command_parts else \"mean\"\n",
    "    # connection_string = os.getenv(\"POSTGES_URL\")\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    conn = create_engine(connection_string)\n",
    "    data = pd.read_sql_query(query, conn)\n",
    "    numerical_cols = data.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    print(features)\n",
    "    flag=0\n",
    "    if features == '*':\n",
    "        if data.isnull().any().any():  # Check if any null values exist\n",
    "            try:\n",
    "                numerical_imputer = SimpleImputer(strategy=strat.lower())\n",
    "                data[numerical_cols] = numerical_imputer.fit_transform(data[numerical_cols])\n",
    "                flag=1\n",
    "            except Exception as e:\n",
    "                response['text'] = f\"Error occurred: {e}\"\n",
    "            try:\n",
    "                categorical_imputer = SimpleImputer(strategy=strat.lower())\n",
    "                data[categorical_cols] = categorical_imputer.fit_transform(data[categorical_cols])\n",
    "                flag=1\n",
    "            except Exception as e:\n",
    "                response['text'] = f\"Error occurred: {e}\"\n",
    "        else:\n",
    "            response['text'] = \"No missing values to impute.\"\n",
    "            return response\n",
    "\n",
    "    \n",
    "    elif features:\n",
    "        if features in numerical_cols:\n",
    "            if data[features].isnull().any(): \n",
    "                numerical_imputer = SimpleImputer(strategy=strat.lower())\n",
    "                data[features] = numerical_imputer.fit_transform(data[[features]])\n",
    "                flag=1\n",
    "            else:\n",
    "                response['text'] = f\"No missing values in {features} .\"\n",
    "                return response\n",
    "        elif features in categorical_cols:\n",
    "            if data[features].isnull().any():\n",
    "                categorical_imputer = SimpleImputer(strategy=strat.lower())\n",
    "                data[features] = categorical_imputer.fit_transform(data[[features]])\n",
    "                flag=1\n",
    "            else:\n",
    "                response['text'] = f\"No missing values in {features}.\"\n",
    "                return response\n",
    "        else:\n",
    "            response['text'] = f\"{features} not exists in {table_name}\"\n",
    "            return response\n",
    "    data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    if flag:\n",
    "        response['text'] = \"Imputation complete\"\n",
    "    print(response)\n",
    "    response\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3,os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def encoding( table_name, cmd):\n",
    "    ''' INSPECT ENCODING USING ONE-HOT feature medv from boston '''\n",
    "    # conn = sqlite3.connect(url)\n",
    "    # connection_string = os.getenv(\"POSTGES_URL\")\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    conn = create_engine(connection_string)\n",
    "    data = pd.read_sql_query(query, conn)\n",
    "    features = cmd[cmd.index(\"INSPECT\") + 1]\n",
    "    method = cmd[cmd.index(\"METHOD\")+ 1] if \"METHOD\" in cmd else \"Ordinal\"\n",
    "    print(method)\n",
    "    response={}\n",
    "    print(method.upper())\n",
    "    if method.upper() == \"ORDINAL\":\n",
    "        response['text']= ordinal_encoding(data, features, cmd, conn, table_name)\n",
    "    elif method.upper() == \"ONE-HOT\":\n",
    "        response['text']= onehot_encoding(data, features, cmd, conn, table_name)\n",
    "    elif method.upper() == \"LABEL\":\n",
    "        response['text']= label_encoding(data, features, cmd, conn, table_name)\n",
    "    elif method.upper() == \"TARGET\":\n",
    "        response['text']= target_encoding(data, features, cmd, conn, table_name)\n",
    "    return response\n",
    "def ordinal_encoding(data, var, cmd, conn, table_name):\n",
    "    \"\"\"INSPECT ENCODING USING Ordinal FEATURE Species FROM Iris;\"\"\"\n",
    "    unique_val = data[var].unique()  \n",
    "    enc_val = range(len(unique_val)) \n",
    "    order = cmd[cmd.index(\"ORDER\") + 1].split(',') if \"ORDER\" in cmd else data[var].unique()\n",
    "    ordinal_enc_dict = {val: new_val for val, new_val in zip(order, enc_val)}\n",
    "    \n",
    "    if len(ordinal_enc_dict) == len(unique_val):\n",
    "        encoder = OrdinalEncoder(categories=[list(ordinal_enc_dict.keys())])\n",
    "        data[var] = encoder.fit_transform(data[[var]])\n",
    "        data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "        print(data)\n",
    "        return f\"Ordinal Encoding Succcessfully Done!\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def onehot_encoding(data, var, cmd, conn, table_name):\n",
    "    \"\"\"INSPECT ENCODING USING One-Hot ENCODING FEATURE Species FROM Iris;\"\"\"\n",
    "    # data = pd.get_dummies(data[var]) it will also return one hot encoded data\n",
    "    encoder = OneHotEncoder()\n",
    "    encoded_data = encoder.fit_transform(data[[var]])\n",
    "    encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out([var]))\n",
    "    data = pd.concat([data, encoded_df], axis=1)\n",
    "    data.drop(columns=[var], inplace=True)\n",
    "    data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    print(data)\n",
    "    return f\"One-hot Encoding Succcessfully Done!\"\n",
    "\n",
    "def label_encoding(data, var, cmd, conn, table_name):\n",
    "    \"\"\"INSPECT ENCODING USING Label ENCODING FEATURE Species FROM Iris;\"\"\"\n",
    "    print('label encode')\n",
    "    label_encoder = LabelEncoder()\n",
    "    data[var] = label_encoder.fit_transform(data[var])\n",
    "    data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    print(data)\n",
    "    return f\"Label Encoding Succcessfully Done!\"\n",
    "\n",
    "def target_encoding(data, cat_var, cmd, conn, table_name):\n",
    "    \"\"\"INSPECT ENCODING USING TARGET ENCODING FEATURE Species  TARGET-FEATURE SepalLengthCm FROM Iris;\"\"\"\n",
    "    try: target_var = cmd[cmd.index(\"TARGET-FEATURE\") + 1] if \"TARGET-FEATURE\" in cmd else None\n",
    "    except ValueError as ve: return None\n",
    "    target_mean = data.groupby(cat_var)[target_var].mean()\n",
    "    print(target_mean)\n",
    "    data[target_var+\"_target_encoded\"] = data[cat_var].map(target_mean)\n",
    "    data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    print(data)\n",
    "    return f\" Target Encoding Succcessfully Done!\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "def deduplicate(command):\n",
    "    response = {'text': \"\"}\n",
    "    command_parts = command\n",
    "    table_name = command_parts[command_parts.index(\"FROM\") + 1].split(';')[0]\n",
    "    feature = command_parts[command_parts.index(\"INSPECT\") + 1]\n",
    "    # connection_string = os.getenv(\"POSTGES_URL\")\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    conn = create_engine(connection_string)\n",
    "    data = pd.read_sql_query(query, conn)\n",
    "\n",
    "    if feature!='*':\n",
    "        if feature in data.columns:\n",
    "            data.drop_duplicates(subset=[feature], inplace=True)\n",
    "            data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "            response['text'] = f\"Deduplication based on feature '{feature}' complete.\"\n",
    "        else:\n",
    "            response['text'] = f\"Feature '{feature}' not found in the table.\"\n",
    "\n",
    "    else:\n",
    "        initial_rows = len(data)\n",
    "        data.drop_duplicates(inplace=True)\n",
    "        final_rows = len(data)\n",
    "        print(final_rows, initial_rows)\n",
    "        if final_rows < initial_rows:\n",
    "            data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "            response['text'] = \"Deduplication based on all features complete.\"\n",
    "        else:\n",
    "            response['text'] = \"No duplicate rows found.\"\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "\n",
    "def checknull(table_name):\n",
    "    try:\n",
    "        response = []\n",
    "        # postgres_url = os.getenv(\"POSTGES_URL\")\n",
    "        connection = psycopg2.connect(connection_string)\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        cursor.execute(f\"SELECT column_name FROM information_schema.columns WHERE table_name = '{table_name}';\")\n",
    "        column_names = [row[0] for row in cursor.fetchall()]\n",
    "        # print(column_names)\n",
    "        cursor.execute(f'SELECT * FROM \"{table_name}\"')\n",
    "        rows = cursor.fetchall()\n",
    "        u = 1\n",
    "        for column_name in column_names:\n",
    "            null_rows = [row for row in rows if row[column_names.index(column_name)] is None]\n",
    "            if null_rows and u:\n",
    "                response = [\"null value exist in column \", \" : \"]\n",
    "                u = 0\n",
    "            if null_rows:\n",
    "                print(f\"  Null values found in {column_name} column:\")\n",
    "                response.append(f\" {column_name},\")\n",
    "                for row in null_rows:\n",
    "                    print(row)\n",
    "            else:\n",
    "                print(f\" in {column_name} column.\")\n",
    "        if u:\n",
    "            response.append(f\"No null values found in {table_name}\")\n",
    "        cursor.close()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error occurred: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import os\n",
    "def categorize(table_name,cmd):\n",
    "    feature=cmd[cmd.index(\"INSPECT\") + 1] \n",
    "    labels=[cat for cat in cmd[cmd.index(\"INTO\") + 1].split(',')]\n",
    "    response={}\n",
    "    # connection_string = os.getenv(\"POSTGES_URL\")\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    conn = create_engine(connection_string)\n",
    "    df=  pd.read_sql(query,conn)\n",
    "    df=pd.DataFrame(df)\n",
    "    min_value = df[feature].min()\n",
    "    max_value = df[feature].max()\n",
    "    print(min_value,max_value)\n",
    "    num_groups = len(labels)\n",
    "    col_range = (max_value - min_value + 1) / num_groups\n",
    "    print(\"res \",col_range)\n",
    "    col_ranges = [(min_value + i * col_range, min_value + (i + 1) * col_range) for i in range(num_groups)] # min_value + (i + 1) * col_range -1 will be like (1,2) (3,4)\n",
    "    col_ranges[-1] = (col_ranges[-1][0], max_value)\n",
    "    # labels = [f\"{label_prefix}-{i+1}\" for i in range(num_groups)]\n",
    "    print(col_ranges)\n",
    "    def assign_label(age):\n",
    "        for i, (start, end) in enumerate(col_ranges):\n",
    "            # print(start,end,age)\n",
    "            if start <= age <= end:\n",
    "                return labels[i]\n",
    "        return 'Unknown'\n",
    "\n",
    "    df['Category'] = df[feature].apply(assign_label)\n",
    "    print(df)\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    response['text']=\"Categorize Done!\"\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "def inspect(command):\n",
    "    command_parts = [part for part in command.split(\" \") if part.strip()]\n",
    "    try:\n",
    "        operation_types = [\"CHECKNULL\", \"ENCODING\",\"DEDUPLICATE\",\"CATEGORIZE\"]\n",
    "        operation_type = next((word for word in operation_types if word in command), \"\") \n",
    "        dataset_name = command_parts[command_parts.index(\"FROM\") + 1].split(';')[0]\n",
    "        features=command_parts[command_parts.index(\"INSPECT\") + 1] #.split(',')\n",
    "          \n",
    "    except:\n",
    "        pass\n",
    "    response={}\n",
    "    # url = os.path.join(os.path.dirname(__file__))\n",
    "    if operation_type.upper()==\"CHECKNULL\":\n",
    "        response['text']= checknull(dataset_name)\n",
    "        return response\n",
    "    elif operation_type.upper() ==\"ENCODING\":\n",
    "        res=encoding(dataset_name,command_parts)\n",
    "        if res: return res\n",
    "        else:\n",
    "            response['text']=\"Something wrong. try again\"\n",
    "            return response\n",
    "    elif operation_type.upper() ==\"DEDUPLICATE\":\n",
    "        res= deduplicate(command_parts)\n",
    "        if res:\n",
    "            return res\n",
    "        else:\n",
    "            response['text']=\"Something wrong. try again\"\n",
    "            return response\n",
    "    elif operation_type.upper() ==\"CATEGORIZE\":\n",
    "        res = categorize(dataset_name,command_parts)\n",
    "        if res:\n",
    "            return res\n",
    "        else:\n",
    "            response['text']=\"Something wrong. try again\"\n",
    "    elif command.startswith(\"IMPUTE\"):\n",
    "        return impute(command)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'No missing values in indus .'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection_string = \"postgresql://postgres:1234@localhost:5432/DL4ML\" \n",
    "\n",
    "command=\"IMPUTE indus  USING STRATEGY mean FROM  BostonMiss;\"\n",
    "inspect(command)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
